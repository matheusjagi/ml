# ğŸ  House Prices â€“ Modelagem de RegressÃ£o com Machine Learning

Este repositÃ³rio contÃ©m um projeto completo de anÃ¡lise, prÃ©-processamento, modelagem e avaliaÃ§Ã£o de modelos de **Machine Learning para regressÃ£o**, utilizando o famoso dataset **House Prices â€“ Advanced Regression Techniques** (Kaggle).  
O objetivo Ã© prever o preÃ§o de venda de imÃ³veis com base em suas caracterÃ­sticas estruturais, de qualidade e localizaÃ§Ã£o.

---

## ğŸ“Œ SumÃ¡rio
- [DescriÃ§Ã£o do Projeto](#descriÃ§Ã£o-do-projeto)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Dataset](#dataset)
- [Etapas do Projeto](#etapas-do-projeto)
- [Modelos Avaliados](#modelos-avaliados)
- [Resultados Obtidos](#resultados-obtidos)
- [AnÃ¡lise dos Erros](#anÃ¡lise-dos-erros)
- [Como Executar](#como-executar)
- [Estrutura do RepositÃ³rio](#estrutura-do-repositÃ³rio)
- [Melhorias Futuras](#melhorias-futuras)
- [LicenÃ§a](#licenÃ§a)

---

## ğŸ“– DescriÃ§Ã£o do Projeto

Este projeto aplica tÃ©cnicas de **aprendizado supervisionado** para resolver o problema de regressÃ£o de previsÃ£o de preÃ§os de casas.

Foram realizadas:
- AnÃ¡lise exploratÃ³ria dos dados (EDA)
- PrÃ©-processamento completo com Pipelines e ColumnTransformer
- TransformaÃ§Ã£o logarÃ­tmica da variÃ¡vel-alvo
- Treinamento de mÃºltiplos modelos
- AvaliaÃ§Ã£o comparativa com mÃ©tricas e cross-validation
- AnÃ¡lise de resÃ­duos e principais erros

---

## ğŸ›  Tecnologias Utilizadas

- Python 3.10+
- Pandas
- NumPy
- Matplotlib / Seaborn
- Scikit-learn
- SciPy
- Jupyter Notebook

---

## ğŸ—‚ Dataset

Dataset: **House Prices â€“ Advanced Regression Techniques** (Kaggle)  
ContÃ©m:
- 1460 registros
- 79 variÃ¡veis preditoras
- VariÃ¡vel-alvo: `SalePrice`

O dataset deve ser baixado manualmente devido Ã s regras do Kaggle:  
ğŸ”— https://www.kaggle.com/c/house-prices-advanced-regression-techniques

---

## ğŸ” Etapas do Projeto

### 1ï¸âƒ£ AnÃ¡lise ExploratÃ³ria (EDA)
- Estudo da distribuiÃ§Ã£o da variÃ¡vel alvo  
- CorrelaÃ§Ãµes entre variÃ¡veis  
- GrÃ¡ficos de dispersÃ£o, boxplots e heatmap  
- VerificaÃ§Ã£o de outliers  
- TransformaÃ§Ã£o `log1p` para reduzir assimetria

### 2ï¸âƒ£ PrÃ©-processamento com Pipelines
- ImputaÃ§Ã£o de valores faltantes  
- PadronizaÃ§Ã£o de variÃ¡veis numÃ©ricas  
- CodificaÃ§Ã£o **ordinal** para `KitchenQual`  
- CodificaÃ§Ã£o **One-Hot** para variÃ¡veis nominais  
- CombinaÃ§Ã£o de transformaÃ§Ãµes via `ColumnTransformer`  
- TransformaÃ§Ã£o do target com `TransformedTargetRegressor`

### 3ï¸âƒ£ Treinamento dos Modelos
Modelos utilizados:
- **LinearRegression**
- **RandomForestRegressor**
- **KNeighborsRegressor**

### 4ï¸âƒ£ AvaliaÃ§Ã£o
MÃ©tricas:
- RMSE  
- MAE  
- RÂ²  
- Cross-validation (KFold 5-fold)

---

## ğŸ“Š Resultados Obtidos

### ğŸ“Œ Desempenho no Conjunto de Teste

| Modelo | RMSE â†“ | MAE â†“ | RÂ² â†‘ |
|-------|--------|--------|------|
| **Linear Regression** | **28.367** | 18.354 | **0.895** |
| Random Forest | 28.461 | **17.752** | 0.894 |
| KNN Regressor | 36.170 | 20.712 | 0.829 |

### ğŸ“Œ Cross-Validation (5-fold)

- **RMSE mÃ©dio:** 46.592  
- **Desvio padrÃ£o:** 40.018  

### ğŸ“Œ ConclusÃµes

- A **RegressÃ£o Linear** obteve o melhor equilÃ­brio geral entre RMSE e RÂ².  
- O **Random Forest** apresentou o menor MAE, indicando bom desempenho para valores medianos.  
- O **KNN** foi prejudicado pela alta dimensionalidade resultante do One-Hot Encoding.  
- Todos os modelos apresentaram tendÃªncia a **subestimar imÃ³veis muito valorizados** (alta cauda no QQ-plot).

---

## ğŸ§ª AnÃ¡lise dos Erros

Foram avaliados:
- DistribuiÃ§Ã£o dos resÃ­duos  
- ResÃ­duo vs Valor Predito  
- QQ-Plot para normalidade  
- Tabela de maiores erros absolutos  

Principais observaÃ§Ãµes:
- ResÃ­duos relativamente bem distribuÃ­dos em valores centrais  
- Heterocedasticidade para imÃ³veis caros  
- SubestimaÃ§Ã£o consistente de casas premium  
- Cauda longa demonstrada no QQ-plot

---
